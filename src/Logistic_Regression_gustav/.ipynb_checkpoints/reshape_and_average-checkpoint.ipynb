{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc50b679-9eff-455e-bfe2-84c9a5b2e3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "#To update the imported files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7268e6d-d405-42cb-b769-1906c0dda04c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from utilities import *\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e8b4625-d0d5-4140-9803-ceff93b93ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading the y-paramter from file, might be changed to y_a etc.\n",
    "train_a = pd.read_parquet('../Data_and_task/A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('../Data_and_task/B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('../Data_and_task/C/train_targets.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01007176-36d4-4044-93eb-c8284b3a7a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading estimated/forecasted training_weather from file\n",
    "X_train_estimated_a = pd.read_parquet('../Data_and_task/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../Data_and_task/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../Data_and_task/C/X_train_estimated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e6290827-1675-4bd6-8557-daa3622cea24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading observed weather from file\n",
    "X_train_observed_a = pd.read_parquet('../Data_and_task/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../Data_and_task/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../Data_and_task/C/X_train_observed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96b86c3d-9bc1-40ca-93f6-7bd5baeddeb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading estimated/forecasted test_weather from file\n",
    "X_test_estimated_a = pd.read_parquet('../Data_and_task/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../Data_and_task/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../Data_and_task/C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42f3fd52-910d-4eb0-ac53-0fcba4b8119b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_train_estimated_a.head()\n",
    "#X_test_estimated_b.head()\n",
    "#X_test_estimated_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22088567-301c-473c-9c71-c069caa7da3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cleaning data such that only wanted features are included\n",
    "selected_features = [\n",
    "    \"date_forecast\", \"clear_sky_rad:W\", \"diffuse_rad:W\", \"direct_rad:W\",\n",
    "    \"absolute_humidity_2m:gm3\", \"effective_cloud_cover:p\",\n",
    "    \"is_day:idx\", \"is_in_shadow:idx\", \"fresh_snow_12h:cm\", \"fresh_snow_1h:cm\",\n",
    "    \"fresh_snow_24h:cm\", \"fresh_snow_3h:cm\", \"fresh_snow_6h:cm\",\n",
    "    \"snow_depth:cm\", \"snow_melt_10min:mm\", \"sun_azimuth:d\", \n",
    "    \"sun_elevation:d\", \"wind_speed_10m:ms\"\n",
    "]\n",
    "\n",
    "#Defining y_fetures, will always be the same\n",
    "y_features = [\"time\", \"pv_measurement\"]\n",
    "\n",
    "X_train_observed_a_clean = X_train_observed_a[selected_features].copy()\n",
    "X_train_estimated_a_clean = X_train_estimated_a[selected_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6e94fed4-bcc4-4067-a544-4e9dfe3e1c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation complete\n"
     ]
    }
   ],
   "source": [
    "#calculating average of x value \n",
    "#NB this calculation takes a long time, and one should feture engineer all dataframes before running to \n",
    "#shorten the runtime\n",
    "X_train_observed_a_average = X_train_observed_a_clean.groupby(X_train_observed_a_clean.index // 4).apply(custom_mean2).reset_index(drop=True)\n",
    "#X_train_observed_b_average = X_train_observed_b.groupby(X_train_observed_b.index // 4).apply(custom_mean2).reset_index(drop=True)\n",
    "#X_train_observed_c_average = X_train_observed_c.groupby(X_train_observed_c.index // 4).apply(custom_mean2).reset_index(drop=True)\n",
    "#check when finished\n",
    "print(\"calculation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "38f685e5-6c57-4ae8-aae6-80712d9e521d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation complete\n"
     ]
    }
   ],
   "source": [
    "X_train_estimated_a_average = X_train_estimated_a_clean.groupby(X_train_estimated_a_clean.index // 4).apply(custom_mean2).reset_index(drop=True)\n",
    "X_train_estimated_b_average = X_test_estimated_b.groupby(X_test_estimated_b.index // 4).apply(custom_mean2).reset_index(drop=True)\n",
    "X_train_estimated_c_average = X_test_estimated_c.groupby(X_test_estimated_c.index // 4).apply(custom_mean2).reset_index(drop=True)\n",
    "#check when finished\n",
    "print(\"calculation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "15b2631d-2621-44d1-81df-d6f45cda2377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export data to exel file\n",
    "file_name2 = \"X_train_observed_a_average.xlsx\" \n",
    "# saving the excel\n",
    "X_train_observed_a_average.to_excel(file_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e1ab81b-f363-4f61-bb11-cf52523ec86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_estimated_a_clean = X_train_observed_a_average.drop(columns = [\"date_forecast\"]).copy()\n",
    "#X_test_estimated_b_clean = X_train_observed_b_average.drop(columns = [\"date_forecast\"]).copy()\n",
    "#X_test_estimated_c_clean = X_train_observed_c_average.drop(columns = [\"date_forecast\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30415cbf-33cd-4df6-ba55-c91095bf01be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 370.14963323421324\n"
     ]
    }
   ],
   "source": [
    "#Training a Linear regression model on X_observed_a and testing it on X_estimated_a and evaluating it on MAE, PURELY for testing!\n",
    "#See below for how its done when submitting for A\n",
    "X_train_observed_a_resized, y_train_observed_a = resize_trainingdata(X_train_observed_a_average, train_a, \"date_forecast\", y_features)\n",
    "X_train_estimated_a_resized, y_train_estimated_a = resize_trainingdata(X_train_estimated_a_average, train_a, \"date_forecast\", y_features)\n",
    "\n",
    "#print(X_train_observed_a_resized)\n",
    "#print(y_train_observed_a)\n",
    "\n",
    "#Scaling the data for more fair comparions and faster convergence, ChatGPT\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_observed_a_resized)\n",
    "X_test_scaled = scaler.fit_transform(X_train_estimated_a_resized)\n",
    "\n",
    "#Training the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_scaled, y_train_observed_a[\"pv_measurement\"])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = reg.predict(X_test_scaled)\n",
    "\n",
    "#Evaluate mean asbolute error\n",
    "mae = mean_absolute_error(y_train_estimated_a[\"pv_measurement\"], y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2b0c881b-90b8-4bb2-a41b-1ecf5e5a680f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_observed_b_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#for b\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train_observed_b_resized, y_train_observed_b \u001b[38;5;241m=\u001b[39m resize_trainingdata(X_train_observed_b_clean, train_b, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_features)\n\u001b[0;32m      3\u001b[0m X_train_estimated_b_resized, y_train_estimated_b \u001b[38;5;241m=\u001b[39m resize_trainingdata(X_train_estimated_b_clean, train_b, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_features)\n\u001b[0;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_observed_b_clean' is not defined"
     ]
    }
   ],
   "source": [
    "#for b\n",
    "X_train_observed_b_resized, y_train_observed_b = resize_trainingdata(X_train_observed_b_clean, train_b, \"date_forecast\", y_features)\n",
    "X_train_estimated_b_resized, y_train_estimated_b = resize_trainingdata(X_train_estimated_b_clean, train_b, \"date_forecast\", y_features)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_observed_b_resized)\n",
    "X_test_scaled = scaler.fit_transform(X_train_estimated_b_resized)\n",
    "\n",
    "y_train_observed_b = remove_NaN_from_Y(y_train_observed_b,\"pv_measurement\")\n",
    "#Training the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_scaled, y_train_observed_b[\"pv_measurement\"])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = reg.predict(X_test_scaled)\n",
    "\n",
    "#Evaluate mean asbolute error\n",
    "mae = mean_absolute_error(y_train_estimated_b[\"pv_measurement\"], y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b9803a8-27a3-40b8-9bca-ca6506df8998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29668, 18)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_observed_a_average.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0a69c04e-2a63-4eb5-8ff0-e79d3bf661f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The DType <class 'numpy.dtype[datetime64]'> could not be promoted by <class 'numpy.dtype[float64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[datetime64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m     12\u001b[0m X_train_a_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train_a)\n\u001b[1;32m---> 13\u001b[0m X_test_a_scaled\u001b[38;5;241m=\u001b[39mscaler\u001b[38;5;241m.\u001b[39mfit_transform(X_test)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#Training the models\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Model A:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m reg_a \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:873\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \n\u001b[0;32m    843\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    872\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 873\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    874\u001b[0m     X,\n\u001b[0;32m    875\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    876\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    877\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    878\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    879\u001b[0m )\n\u001b[0;32m    880\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:797\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    793\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    794\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[0;32m    795\u001b[0m )\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 797\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdtypes_orig)\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[0;32m    800\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The DType <class 'numpy.dtype[datetime64]'> could not be promoted by <class 'numpy.dtype[float64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[datetime64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>)"
     ]
    }
   ],
   "source": [
    "#Training model, running infernece on testing data and saving it in a csv file ready for submission\n",
    "X_train_observed_a_resized, y_train_observed_a = resize_trainingdata(X_train_observed_a_average, train_a, \"date_forecast\", y_features)\n",
    "X_train_estimated_a_resized, y_train_estimated_a = resize_trainingdata(X_train_estimated_a_average, train_a, \"date_forecast\", y_features)\n",
    "\n",
    "#Merging observed and estimated data to one big df\n",
    "X_train_a = pd.concat([X_train_observed_a_resized, X_train_estimated_a_resized], ignore_index=True)\n",
    "y_train_a = pd.concat([y_train_observed_a, y_train_estimated_a], ignore_index=True)\n",
    "X_test = pd.concat([X_train_estimated_a_average, X_train_estimated_b_average, X_train_estimated_c_average], ignore_index=True)\n",
    "\n",
    "#Scaling the data for more fair comparions and faster convergence, ChatGPT\n",
    "scaler = StandardScaler()\n",
    "X_train_a_scaled = scaler.fit_transform(X_train_a)\n",
    "X_test_a_scaled=scaler.fit_transform(X_test)\n",
    "\n",
    "#Training the models\n",
    "#Model A:\n",
    "reg_a = LinearRegression()\n",
    "reg_a.fit(X_train_a, y_train_a[\"pv_measurement\"])\n",
    "\n",
    "# Make predictions for A,B,C on the model trained on only A. Problems with the B and C datasets which need to be explored\n",
    "y_pred = reg_a.predict(X_test_a_scaled)\n",
    "\n",
    "#Saving y_pred in a proper csv file\n",
    "filename = \"CSV/second_submission.csv\"\n",
    "submission(filename, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b049e6d4-9f90-4be0-a23e-466b5f02f7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4394, 2)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_estimated_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414af49a-6ae4-46b0-82a8-5cb7f7bebc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ef57d-b9d2-4aeb-970e-ef98de509e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

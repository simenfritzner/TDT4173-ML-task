{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "dc50b679-9eff-455e-bfe2-84c9a5b2e3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "#To update the imported files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "e7268e6d-d405-42cb-b769-1906c0dda04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from utilities import *\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "8e8b4625-d0d5-4140-9803-ceff93b93ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the y-paramter from file, might be changed to y_a etc.\n",
    "train_a = pd.read_parquet('../Data_and_task/A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('../Data_and_task/B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('../Data_and_task/C/train_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "01007176-36d4-4044-93eb-c8284b3a7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading estimated/forecasted training_weather from file\n",
    "X_train_estimated_a = pd.read_parquet('../Data_and_task/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../Data_and_task/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../Data_and_task/C/X_train_estimated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "e6290827-1675-4bd6-8557-daa3622cea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading observed weather from file\n",
    "X_train_observed_a = pd.read_parquet('../Data_and_task/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../Data_and_task/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../Data_and_task/C/X_train_observed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "96b86c3d-9bc1-40ca-93f6-7bd5baeddeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading estimated/forecasted test_weather from file\n",
    "X_test_estimated_a = pd.read_parquet('../Data_and_task/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../Data_and_task/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../Data_and_task/C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "5eb1e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data such that only wanted features are included\n",
    "\n",
    "selected_features = [\"date_forecast\", \"direct_rad:W\", \"direct_rad_1h:J\", \"clear_sky_rad:W\", \"clear_sky_energy_1h:J\", \"diffuse_rad:W\",\n",
    "\"diffuse_rad_1h:J\", \"sun_elevation:d\", \"is_in_shadow:idx\", \"is_day:idx\"]\n",
    "\n",
    "#Defining y_fetures, will always be the same\n",
    "y_features = [\"time\", \"pv_measurement\"]\n",
    "\n",
    "#Fjernen kvarter-radene og fjerner \"date_calc\" kolonnen hvis den finnes\n",
    "X_train_observed_a_clean = clean_X_data(X_train_observed_a)\n",
    "X_train_observed_b_clean = clean_X_data(X_train_observed_b)\n",
    "X_train_observed_c_clean = clean_X_data(X_train_observed_c)\n",
    "X_train_estimated_a_clean = clean_X_data(X_train_observed_a)\n",
    "X_train_estimated_b_clean = clean_X_data(X_train_observed_b)\n",
    "X_train_estimated_c_clean = clean_X_data(X_train_observed_c)\n",
    "X_test_estimated_a_clean = clean_X_data(X_test_estimated_a)\n",
    "X_test_estimated_b_clean = clean_X_data(X_test_estimated_b)\n",
    "X_test_estimated_c_clean = clean_X_data(X_test_estimated_c)\n",
    "\n",
    "X_train_observed_a_clean_selected_features = X_train_observed_a_clean[selected_features].copy()\n",
    "X_train_observed_b_clean_selected_features = X_train_observed_b_clean[selected_features].copy()\n",
    "X_train_observed_c_clean_selected_features = X_train_observed_c_clean[selected_features].copy()\n",
    "X_train_estimated_a_clean_selected_features = X_train_estimated_a_clean[selected_features].copy()\n",
    "X_train_estimated_b_clean_selected_features = X_train_estimated_b_clean[selected_features].copy()\n",
    "X_train_estimated_c_clean_selected_features = X_train_estimated_c_clean[selected_features].copy()\n",
    "X_test_estimated_a_clean_selected_features = X_test_estimated_a_clean[selected_features].copy()\n",
    "X_test_estimated_b_clean_selected_features = X_test_estimated_b_clean[selected_features].copy()\n",
    "X_test_estimated_c_clean_selected_features = X_test_estimated_c_clean[selected_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "8ea93e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Calculate time lapse since a reference time (e.g., the minimum date_forecast)\\nmin_date_forecast = df['date_forecast'].min()\\ndf['delta_time'] = (df['date_forecast'] - min_date_forecast).dt.total_seconds()\\n\\n# Create cyclic month features\\ndf['sine_month'] = np.sin((df['date_forecast'].dt.month - 1) * np.pi / 6)  # Assuming months are 1 to 12\\ndf['cos_month'] = np.cos((df['date_forecast'].dt.month - 1) * np.pi / 6)\\n\\n# Create cyclic hour features\\nmin_hour_of_interest = 10\\nmax_hour_of_interest = 15\\ndf['delta_hour'] = df['date_forecast'].dt.hour - min_hour_of_interest\\ndf['sine_hour'] = np.sin((df['delta_hour'] * np.pi / (max_hour_of_interest - min_hour_of_interest)))\\ndf['cos_hour'] = np.cos((df['delta_hour'] * np.pi / (max_hour_of_interest - min_hour_of_interest)))\\n\""
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Calculate time lapse since a reference time (e.g., the minimum date_forecast)\n",
    "min_date_forecast = df['date_forecast'].min()\n",
    "df['delta_time'] = (df['date_forecast'] - min_date_forecast).dt.total_seconds()\n",
    "\n",
    "# Create cyclic month features\n",
    "df['sine_month'] = np.sin((df['date_forecast'].dt.month - 1) * np.pi / 6)  # Assuming months are 1 to 12\n",
    "df['cos_month'] = np.cos((df['date_forecast'].dt.month - 1) * np.pi / 6)\n",
    "\n",
    "# Create cyclic hour features\n",
    "min_hour_of_interest = 10\n",
    "max_hour_of_interest = 15\n",
    "df['delta_hour'] = df['date_forecast'].dt.hour - min_hour_of_interest\n",
    "df['sine_hour'] = np.sin((df['delta_hour'] * np.pi / (max_hour_of_interest - min_hour_of_interest)))\n",
    "df['cos_hour'] = np.cos((df['delta_hour'] * np.pi / (max_hour_of_interest - min_hour_of_interest)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "b0d375b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 245.43624911612486\n"
     ]
    }
   ],
   "source": [
    "#Training a Linear regression model on X_observed_a and testing it on X_estimated_a and evaluating it on MAE, PURELY for testing!\n",
    "#See below for how its done when submitting\n",
    "X_train_observed_a_clean_selected_features_resized, train_a_observed_resized = resize_trainingdata(X_train_observed_a_clean_selected_features, train_a, \"date_forecast\", y_features)\n",
    "X_train_estimated_a_clean_selected_features_resized, train_a_estimated_resized = resize_trainingdata(X_train_estimated_a_clean_selected_features, train_a, \"date_forecast\", y_features)\n",
    "\n",
    "#Scaling the data for more fair comparions and faster convergence, ChatGPT\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_observed_a_clean_selected_features_resized)\n",
    "X_test_scaled = scaler.fit_transform(X_train_estimated_a_clean_selected_features_resized)\n",
    "\n",
    "#Training the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_scaled, train_a_observed_resized[\"pv_measurement\"])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = reg.predict(X_test_scaled)\n",
    "\n",
    "#Evaluate mean asbolute error\n",
    "mae = mean_absolute_error(train_a_estimated_resized[\"pv_measurement\"], y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "97a22f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>direct_rad:W</th>\n",
       "      <th>direct_rad_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>diffuse_rad_1h:J</th>\n",
       "      <th>sun_elevation:d</th>\n",
       "      <th>is_in_shadow:idx</th>\n",
       "      <th>is_day:idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.202000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.393000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.910000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.986000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6546.899902</td>\n",
       "      <td>4.3</td>\n",
       "      <td>7743.299805</td>\n",
       "      <td>1.401000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29662</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-27.830000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29663</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-32.546001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29664</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-35.674000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29665</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-36.821999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29666</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-36.306000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29667 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       direct_rad:W  direct_rad_1h:J  clear_sky_rad:W  clear_sky_energy_1h:J  \\\n",
       "0               0.0              0.0              0.0               0.000000   \n",
       "1               0.0              0.0              0.0               0.000000   \n",
       "2               0.0              0.0              0.0               0.000000   \n",
       "3               0.0              0.0              0.0               0.000000   \n",
       "4               0.0              0.0              9.8            6546.899902   \n",
       "...             ...              ...              ...                    ...   \n",
       "29662           0.0              0.0              0.0               0.000000   \n",
       "29663           0.0              0.0              0.0               0.000000   \n",
       "29664           0.0              0.0              0.0               0.000000   \n",
       "29665           0.0              0.0              0.0               0.000000   \n",
       "29666           0.0              0.0              0.0               0.000000   \n",
       "\n",
       "       diffuse_rad:W  diffuse_rad_1h:J  sun_elevation:d  is_in_shadow:idx  \\\n",
       "0                0.0          0.000000        -3.202000               1.0   \n",
       "1                0.0          0.000000        -4.393000               1.0   \n",
       "2                0.0          0.000000        -3.910000               1.0   \n",
       "3                0.0          0.000000        -1.986000               1.0   \n",
       "4                4.3       7743.299805         1.401000               0.0   \n",
       "...              ...               ...              ...               ...   \n",
       "29662            0.0          0.000000       -27.830000               1.0   \n",
       "29663            0.0          0.000000       -32.546001               1.0   \n",
       "29664            0.0          0.000000       -35.674000               1.0   \n",
       "29665            0.0          0.000000       -36.821999               1.0   \n",
       "29666            0.0          0.000000       -36.306000               1.0   \n",
       "\n",
       "       is_day:idx  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             1.0  \n",
       "...           ...  \n",
       "29662         0.0  \n",
       "29663         0.0  \n",
       "29664         0.0  \n",
       "29665         0.0  \n",
       "29666         0.0  \n",
       "\n",
       "[29667 rows x 9 columns]"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_estimated_a_clean_selected_features_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "7da1c520",
   "metadata": {},
   "outputs": [
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float32DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDTypePromotionError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/erik/Library/CloudStorage/OneDrive-NTNU/Maskinlæring/Oppgave 2 gruppe/TDT4173-ML-task/src/Logistic_Regression/Linear_regression.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erik/Library/CloudStorage/OneDrive-NTNU/Maskinl%C3%A6ring/Oppgave%202%20gruppe/TDT4173-ML-task/src/Logistic_Regression/Linear_regression.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erik/Library/CloudStorage/OneDrive-NTNU/Maskinl%C3%A6ring/Oppgave%202%20gruppe/TDT4173-ML-task/src/Logistic_Regression/Linear_regression.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m X_train_a_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X_train_a)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/erik/Library/CloudStorage/OneDrive-NTNU/Maskinl%C3%A6ring/Oppgave%202%20gruppe/TDT4173-ML-task/src/Logistic_Regression/Linear_regression.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X_test_a_scaled\u001b[39m=\u001b[39mscaler\u001b[39m.\u001b[39;49mfit_transform(X_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erik/Library/CloudStorage/OneDrive-NTNU/Maskinl%C3%A6ring/Oppgave%202%20gruppe/TDT4173-ML-task/src/Logistic_Regression/Linear_regression.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#Training the models\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erik/Library/CloudStorage/OneDrive-NTNU/Maskinl%C3%A6ring/Oppgave%202%20gruppe/TDT4173-ML-task/src/Logistic_Regression/Linear_regression.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#Model A:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erik/Library/CloudStorage/OneDrive-NTNU/Maskinl%C3%A6ring/Oppgave%202%20gruppe/TDT4173-ML-task/src/Logistic_Regression/Linear_regression.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m reg_a \u001b[39m=\u001b[39m LinearRegression()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:873\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \n\u001b[1;32m    843\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    872\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 873\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    874\u001b[0m     X,\n\u001b[1;32m    875\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    876\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    877\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    878\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    880\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:797\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    793\u001b[0m pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[1;32m    794\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[1;32m    795\u001b[0m )\n\u001b[1;32m    796\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[0;32m--> 797\u001b[0m     dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mresult_type(\u001b[39m*\u001b[39;49mdtypes_orig)\n\u001b[1;32m    798\u001b[0m \u001b[39melif\u001b[39;00m pandas_requires_conversion \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(d \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    799\u001b[0m     \u001b[39m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[1;32m    800\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\n",
      "\u001b[0;31mDTypePromotionError\u001b[0m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float32DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>)"
     ]
    }
   ],
   "source": [
    "#Training model, running infernece on testing data and saving it in a csv file ready for submission\n",
    "X_train_observed_a_clean_selected_features_resized, train_a_observed_resized = resize_trainingdata(X_train_observed_a_clean_selected_features, train_a, \"date_forecast\", y_features)\n",
    "X_train_estimated_a_clean_selected_features_resized, train_a_estimated_resized = resize_trainingdata(X_train_estimated_a_clean_selected_features, train_a, \"date_forecast\", y_features)\n",
    "\n",
    "#Merging observed and estimated data to one big df\n",
    "X_train_a = pd.concat([X_train_observed_a_clean_selected_features_resized, X_train_estimated_a_clean_selected_features_resized], ignore_index=True)\n",
    "y_train_a = pd.concat([train_a_observed_resized, train_a_estimated_resized], ignore_index=True)\n",
    "X_test = pd.concat([X_test_estimated_a_clean_selected_features, X_test_estimated_b_clean_selected_features, X_test_estimated_c_clean_selected_features], ignore_index=True)\n",
    "\n",
    "#Scaling the data for more fair comparions and faster convergence, ChatGPT\n",
    "scaler = StandardScaler()\n",
    "X_train_a_scaled = scaler.fit_transform(X_train_a)\n",
    "X_test_a_scaled=scaler.fit_transform(X_test)\n",
    "\n",
    "#Training the models\n",
    "#Model A:\n",
    "reg_a = LinearRegression()\n",
    "reg_a.fit(X_train_a, y_train_a[\"pv_measurement\"])\n",
    "\n",
    "# Make predictions for A,B,C on the model trained on only A. Problems with the B and C datasets which need to be explored\n",
    "y_pred = reg_a.predict(X_test_a_scaled)\n",
    "\n",
    "#Saving y_pred in a proper csv file\n",
    "filename = \"CSV/second_submission.csv\"\n",
    "submission(filename, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c7a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "#To update the imported files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337eba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from utilities import *\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c507d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the y-paramter from file, might be changed to y_a etc.\n",
    "train_a = pd.read_parquet('../Data_and_task/A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('../Data_and_task/B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('../Data_and_task/C/train_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbe712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading estimated/forecasted training_weather from file\n",
    "X_train_estimated_a = pd.read_parquet('../Data_and_task/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../Data_and_task/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../Data_and_task/C/X_train_estimated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading observed weather from file\n",
    "X_train_observed_a = pd.read_parquet('../Data_and_task/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../Data_and_task/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../Data_and_task/C/X_train_observed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading estimated/forecasted test_weather from file\n",
    "X_test_estimated_a = pd.read_parquet('../Data_and_task/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../Data_and_task/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../Data_and_task/C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data such that only wanted features are included\n",
    "\n",
    "selected_features = [\"date_forecast\", \"direct_rad:W\", \"direct_rad_1h:J\", \"clear_sky_rad:W\", \"clear_sky_energy_1h:J\", \"diffuse_rad:W\",\n",
    "\"diffuse_rad_1h:J\", \"sun_elevation:d\", \"is_in_shadow:idx\", \"is_day:idx\"]\n",
    "\n",
    "#Defining y_fetures, will always be the same\n",
    "y_features = [\"time\", \"pv_measurement\"]\n",
    "\n",
    "#Fjernen kvarter-radene og fjerner \"date_calc\" kolonnen hvis den finnes\n",
    "X_train_observed_a_clean = clean_X_data(X_train_observed_a)\n",
    "X_train_observed_b_clean = clean_X_data(X_train_observed_b)\n",
    "X_train_observed_c_clean = clean_X_data(X_train_observed_c)\n",
    "X_train_estimated_a_clean = clean_X_data(X_train_observed_a)\n",
    "X_train_estimated_b_clean = clean_X_data(X_train_observed_b)\n",
    "X_train_estimated_c_clean = clean_X_data(X_train_observed_c)\n",
    "X_test_estimated_a_clean = clean_X_data(X_test_estimated_a)\n",
    "X_test_estimated_b_clean = clean_X_data(X_test_estimated_b)\n",
    "X_test_estimated_c_clean = clean_X_data(X_test_estimated_c)\n",
    "\n",
    "X_train_observed_a_clean_selected_features = X_train_observed_a_clean[selected_features].copy()\n",
    "X_train_observed_b_clean_selected_features = X_train_observed_b_clean[selected_features].copy()\n",
    "X_train_observed_c_clean_selected_features = X_train_observed_c_clean[selected_features].copy()\n",
    "X_train_estimated_a_clean_selected_features = X_train_estimated_a_clean[selected_features].copy()\n",
    "X_train_estimated_b_clean_selected_features = X_train_estimated_b_clean[selected_features].copy()\n",
    "X_train_estimated_c_clean_selected_features = X_train_estimated_c_clean[selected_features].copy()\n",
    "X_test_estimated_a_clean_selected_features = X_test_estimated_a_clean[selected_features].copy()\n",
    "X_test_estimated_b_clean_selected_features = X_test_estimated_b_clean[selected_features].copy()\n",
    "X_test_estimated_c_clean_selected_features = X_test_estimated_c_clean[selected_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad264a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 245.43624911612486\n"
     ]
    }
   ],
   "source": [
    "#Training a Linear regression model on X_observed_a and testing it on X_estimated_a and evaluating it on MAE, PURELY for testing!\n",
    "#See below for how its done when submitting\n",
    "X_train_observed_a_clean_selected_features_resized, train_a_observed_resized = resize_trainingdata(X_train_observed_a_clean_selected_features, train_a, \"date_forecast\", y_features)\n",
    "X_train_estimated_a_clean_selected_features_resized, train_a_estimated_resized = resize_trainingdata(X_train_estimated_a_clean_selected_features, train_a, \"date_forecast\", y_features)\n",
    "\n",
    "#Scaling the data for more fair comparions and faster convergence, ChatGPT\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_observed_a_clean_selected_features_resized)\n",
    "X_test_scaled = scaler.fit_transform(X_train_estimated_a_clean_selected_features_resized)\n",
    "\n",
    "#Training the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_scaled, train_a_observed_resized[\"pv_measurement\"])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = reg.predict(X_test_scaled)\n",
    "\n",
    "#Evaluate mean asbolute error\n",
    "mae = mean_absolute_error(train_a_estimated_resized[\"pv_measurement\"], y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
